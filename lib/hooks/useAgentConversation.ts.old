/**
 * useAgentConversation Hook
 * Manages client-side WebSocket connection to ElevenLabs Agent with voice output
 */

import { useState, useEffect, useRef, useCallback } from 'react';

const ELEVENLABS_API_KEY = process.env.NEXT_PUBLIC_ELEVENLABS_API_KEY || '';
const AGENT_ID = process.env.NEXT_PUBLIC_ELEVENLABS_AGENT_ID || '';

export interface UseAgentConversationResult {
  isConnected: boolean;
  isSpeaking: boolean;
  agentResponse: string;
  sendMessage: (text: string) => void;
  sendAudio: (audioData: ArrayBuffer) => void;
  startConversation: () => Promise<void>;
  endConversation: () => void;
  clearAgentResponse: () => void;
}

export function useAgentConversation(): UseAgentConversationResult {
  const [isConnected, setIsConnected] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [agentResponse, setAgentResponse] = useState('');
  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const audioQueueRef = useRef<AudioBuffer[]>([]);
  const isPlayingRef = useRef(false);

  // Play audio queue
  const playNextAudio = useCallback(() => {
    if (isPlayingRef.current) {
      return;
    }

    if (audioQueueRef.current.length === 0) {
      return;
    }

    const audioBuffer = audioQueueRef.current.shift();
    if (!audioBuffer || !audioContextRef.current) {
      console.error('‚ùå No audio buffer or context');
      return;
    }

    console.log('‚ñ∂Ô∏è Playing Eric\'s audio:', audioBuffer.duration.toFixed(2), 'seconds');
    isPlayingRef.current = true;
    setIsSpeaking(true);

    const source = audioContextRef.current.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(audioContextRef.current.destination);

    source.onended = () => {
      console.log('‚úÖ Audio finished');
      isPlayingRef.current = false;
      if (audioQueueRef.current.length === 0) {
        setIsSpeaking(false);
      } else {
        playNextAudio();
      }
    };

    source.start(0);
  }, []);

  // Initialize Web Audio Context
  useEffect(() => {
    audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
    console.log('üéµ Audio Context created, sample rate:', audioContextRef.current.sampleRate);

    return () => {
      audioContextRef.current?.close();
    };
  }, []);

  // Process incoming audio chunk (raw PCM base64 string)
  const processAudioChunk = useCallback((base64Audio: string) => {
    console.log('üîä Processing RAW PCM audio chunk from Eric');

    try {
      if (!audioContextRef.current) {
        console.error('‚ùå No audio context');
        return;
      }

      // Decode base64 to binary
      const binaryString = atob(base64Audio);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }

      console.log('üì¶ Decoded', bytes.length, 'bytes of raw PCM');

      // ElevenLabs sends 16-bit PCM, convert to Int16Array
      const int16Array = new Int16Array(bytes.buffer);
      const numSamples = int16Array.length;

      console.log('üéµ Converted to', numSamples, 'PCM samples');

      // Try 16kHz (common for speech) - ElevenLabs likely uses this
      const sampleRate = 16000;
      const numChannels = 1;

      // Create AudioBuffer
      const audioBuffer = audioContextRef.current.createBuffer(
        numChannels,
        numSamples,
        sampleRate
      );

      // Get the audio buffer's channel data
      const channelData = audioBuffer.getChannelData(0);

      // Convert 16-bit PCM to float32 (-1.0 to 1.0)
      for (let i = 0; i < numSamples; i++) {
        channelData[i] = int16Array[i] / 32768.0; // Normalize 16-bit to -1.0 to 1.0
      }

      console.log('‚úÖ Created AudioBuffer:', audioBuffer.duration.toFixed(2), 'seconds at', sampleRate, 'Hz');

      audioQueueRef.current.push(audioBuffer);
      playNextAudio();
    } catch (error) {
      console.error('‚ùå Error processing PCM audio:', error);
    }
  }, [playNextAudio]);

  // Get signed WebSocket URL
  const getSignedUrl = async (): Promise<string> => {
    console.log('üì° Requesting signed URL for agent:', AGENT_ID);

    const response = await fetch(
      `https://api.elevenlabs.io/v1/convai/conversation/get-signed-url?agent_id=${AGENT_ID}`,
      {
        headers: {
          'xi-api-key': ELEVENLABS_API_KEY,
        },
      }
    );

    if (!response.ok) {
      const error = await response.text();
      console.error('‚ùå Failed to get signed URL:', response.status, error);
      throw new Error(`Failed to get signed URL: ${response.status}`);
    }

    const data = await response.json();
    console.log('‚úÖ Got signed URL');
    return data.signed_url;
  };

  // Start conversation
  const startConversation = useCallback(async () => {
    try {
      console.log('üéôÔ∏è Starting agent conversation...');

      const signedUrl = await getSignedUrl();
      const ws = new WebSocket(signedUrl);
      wsRef.current = ws;

      ws.onopen = () => {
        console.log('‚úÖ Agent connected');
        setIsConnected(true);
        audioChunkCountRef.current = 0; // Reset audio chunk counter

        // Use agent's dashboard configuration (prompt, voice, first message)
        const config = {
          type: 'conversation_initiation_client_data',
          conversation_config_override: {
            conversation: {
              text_only: false, // Enable voice
            },
          },
        };

        console.log('üì§ Using Eric\'s dashboard configuration');
        ws.send(JSON.stringify(config));
      };

      ws.onmessage = async (event) => {
        console.log('üì• WebSocket message received, type:', typeof event.data);

        // Handle binary audio data (shouldn't happen with ElevenLabs text-mode protocol)
        if (event.data instanceof Blob) {
          console.log('üîä Received audio Blob:', event.data.size, 'bytes');
          // Convert blob to base64 and process
          const arrayBuffer = await event.data.arrayBuffer();
          const base64 = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));
          processAudioChunk(base64);
          return;
        }

        // Handle text messages
        try {
          const message = JSON.parse(event.data);
          console.log('üì• Agent message type:', message.type);

          // Capture agent text responses (correct field: text_response_part)
          if (message.type === 'agent_chat_response_part' && message.text_response_part?.text) {
            console.log('üí¨ Eric said:', message.text_response_part.text);
            setAgentResponse(prev => prev + message.text_response_part.text + ' ');
          }

          // Handle audio events (correct field: audio_event.audio_base_64)
          if (message.type === 'audio' && message.audio_event?.audio_base_64) {
            console.log('üîä Received base64 audio from Eric');
            // Pass base64 string directly
            processAudioChunk(message.audio_event.audio_base_64);
          }
        } catch (error) {
          console.error('‚ùå Error parsing message:', error);
        }
      };

      ws.onerror = (error) => {
        console.error('‚ùå WebSocket error:', error);
      };

      ws.onclose = () => {
        console.log('üîå Agent disconnected');
        setIsConnected(false);
        setIsSpeaking(false);
      };

    } catch (error) {
      console.error('‚ùå Failed to start conversation:', error);
      setIsConnected(false);
    }
  }, [processAudioChunk]);

  // Send message to agent
  const sendMessage = useCallback((text: string) => {
    if (wsRef.current?.readyState === WebSocket.OPEN) {
      console.log('üì§ Sending text to agent:', text);
      wsRef.current.send(JSON.stringify({
        type: 'user_message',
        text,
      }));
    }
  }, []);

  // Send audio to agent (for real-time listening)
  const audioChunkCountRef = useRef(0);
  const sendAudio = useCallback((audioData: ArrayBuffer) => {
    if (wsRef.current?.readyState === WebSocket.OPEN) {
      // Convert ArrayBuffer to base64
      const uint8Array = new Uint8Array(audioData);
      const binaryString = String.fromCharCode(...uint8Array);
      const base64Audio = btoa(binaryString);

      audioChunkCountRef.current++;
      // Log every 10th chunk to avoid spam
      if (audioChunkCountRef.current % 10 === 0) {
        console.log('üé§ Sent', audioChunkCountRef.current, 'audio chunks to Eric (', audioData.byteLength, 'bytes in this chunk)');
      }

      wsRef.current.send(JSON.stringify({
        type: 'user_audio_chunk',
        audio_chunk: {
          audio_base_64: base64Audio,
        },
      }));
    } else {
      console.warn('‚ö†Ô∏è WebSocket not open, cannot send audio. State:', wsRef.current?.readyState);
    }
  }, []);

  // End conversation
  const endConversation = useCallback(() => {
    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }
    audioQueueRef.current = [];
    isPlayingRef.current = false;
    setIsConnected(false);
    setIsSpeaking(false);
    setAgentResponse('');
  }, []);

  // Clear agent response
  const clearAgentResponse = useCallback(() => {
    setAgentResponse('');
  }, []);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      endConversation();
    };
  }, [endConversation]);

  return {
    isConnected,
    isSpeaking,
    agentResponse,
    sendMessage,
    sendAudio,
    startConversation,
    endConversation,
    clearAgentResponse,
  };
}
